<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>此间不留白</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://elmadavies.github.io/"/>
  <updated>2019-10-17T10:21:05.084Z</updated>
  <id>https://elmadavies.github.io/</id>
  
  <author>
    <name>ElmaDavies</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>deeplearning 课后作业1</title>
    <link href="https://elmadavies.github.io/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AE%9E%E8%B7%B51/"/>
    <id>https://elmadavies.github.io/深度学习/神经网络实践1/</id>
    <published>2019-10-17T09:37:40.350Z</published>
    <updated>2019-10-17T10:21:05.084Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><blockquote><p>deeplearning.ai官网地址：<a href="https://www.deeplearning.ai/" target="_blank" rel="noopener">https://www.deeplearning.ai/</a><br>coursera地址：<a href="https://www.coursera.org/specializations/deep-learning" target="_blank" rel="noopener">https://www.coursera.org/specializations/deep-learning</a><br>网易视频地址：<a href="https://163.lu/nPtn42" target="_blank" rel="noopener">https://163.lu/nPtn42</a><br>课程一第二周课后作业1-1</p></blockquote><h1 id="使用Numpy构造基础数学函数"><a href="#使用Numpy构造基础数学函数" class="headerlink" title="使用Numpy构造基础数学函数"></a>使用Numpy构造基础数学函数</h1><p>numpy是一个python中基础的科学计算包，以下练习中，将会实现一些数学中的基础函数，如<code>np.log(),np.exp()</code>等。</p><p>对于机器学习中的逻辑回归激活函数，表达式为$\delta = \frac{1}{1+e^{-z}}$，对于此公式，考虑到参数$z$是一个实数，可以简单的使用python中的<code>math</code>实现。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(z)</span>:</span></span><br><span class="line">  <span class="keyword">return</span> <span class="number">1</span>/(<span class="number">1</span>+math.exp(-z))</span><br></pre></td></tr></table></figure><p>以上实现中，实际能够传入的参数<code>z</code>只能是一个数字，而在机器学习或者深度学习中，实际需要传入的参数是一个向量，所以<code>math.exp()</code>并不适用，可以使用<code>numpy.exp()</code>实现，如下所示：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(z)</span>:</span></span><br><span class="line">  sigmoid = <span class="number">1</span>/(<span class="number">1</span>+np.exp(-z))</span><br><span class="line">  <span class="keyword">return</span> sigmoid</span><br></pre></td></tr></table></figure><p>对于一个向量而言，<code>np.exp()</code>的实现过程就是对向量中的每一个元素计算<code>np.exp()</code>,可以如下图表示:</p><p><a href="https://elamdavies-1300381401.cos.ap-chengdu.myqcloud.com/deepLearning_course1/%E8%AF%BE%E7%A8%8B1-%E7%AC%AC%E4%BA%8C%E5%91%A8%E4%BD%9C%E4%B8%9A1-1.png" target="_blank" rel="noopener">https://elamdavies-1300381401.cos.ap-chengdu.myqcloud.com/deepLearning_course1/%E8%AF%BE%E7%A8%8B1-%E7%AC%AC%E4%BA%8C%E5%91%A8%E4%BD%9C%E4%B8%9A1-1.png</a></p><h2 id="实现激活函数梯度"><a href="#实现激活函数梯度" class="headerlink" title="实现激活函数梯度"></a>实现激活函数梯度</h2><p>在之前的学习中，需要计算梯度优化使用反向传播算法的损失函数，对于此梯度的计算公式可以如下表示：<br>$\delta’(z) = \delta(z)*(1-\delta(z))$</p><p>代码实如下所示：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid_derivative</span><span class="params">(z)</span>:</span></span><br><span class="line">  s = sigmoid(z)</span><br><span class="line">  ds = s(<span class="number">1</span>-s)</span><br><span class="line">  <span class="keyword">return</span> ds</span><br></pre></td></tr></table></figure><h2 id="改变矩阵（向量）形状"><a href="#改变矩阵（向量）形状" class="headerlink" title="改变矩阵（向量）形状"></a>改变矩阵（向量）形状</h2><p>关于深度学习中，numpy中最常用到的两个跟矩阵或者向量形状有关的函数是<code>shape()</code>和<code>reshape()</code>。</p><ol><li><code>shape()</code>：返回当前矩阵或者向量的大小。</li><li><code>reshape()</code>:将当前数组转化为特定形状。</li></ol><p>具体一些用法如下所示;<br>将一个三维图像矩阵(shape=(length,height,depth))，转化为一维数组(shape = (lenght×height×depth,1))以便于处理，具体实现方法可以由以下代码所示：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">image2vector</span><span class="params">(image)</span>:</span></span><br><span class="line">    v = image.reshape(image.shape[<span class="number">0</span>] * image.shape[<span class="number">1</span>] * image.shape[<span class="number">2</span>], <span class="number">1</span>)  </span><br><span class="line">    <span class="keyword">return</span> v</span><br></pre></td></tr></table></figure><h2 id="按行标准化："><a href="#按行标准化：" class="headerlink" title="按行标准化："></a>按行标准化：</h2><p>机器学习或者深度学习中，常常用到的一个数据预处理技巧是标准化，标准化后的数据能够使得算法运行速度更快，关于数据标准化的公式如下所示：</p><p>$normal = \frac{x}{||x||}$</p><p>其中，$||x||$表示范数，对于一个向量$[x_1,x_2……x_n]$，其范数计</p><p>算公式为$\sqrt{x_1^{2}+x_2^{2}+···x_n^{2}}$。</p><p><code>numpy</code>已经提供了用于计算范数的函数，所以其数据标准化的处理过程可以用以下代码实现：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">normalizeRows</span><span class="params">(x)</span>:</span></span><br><span class="line">    x_norm = np.linalg.norm(x, axis = <span class="number">1</span>, keepdims = <span class="literal">True</span>)</span><br><span class="line">    x = x / x_norm</span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><h2 id="Python广播机制和-softmax-函数"><a href="#Python广播机制和-softmax-函数" class="headerlink" title="Python广播机制和$softmax$函数"></a>Python广播机制和$softmax$函数</h2><p>为了理解<code>python</code>中的广播机制，一种行之有效的方法是操作两个维数不同的矩阵。使用<code>numpy</code>中的<code>softmax</code>函数理解<code>python</code>中的广播机制，可以将<code>softmax</code>函数认为是你的算法需要实现二分类或者更多分类时的数据标准化函数，其数学表示如下所示：</p><p><img src="https://elamdavies-1300381401.cos.ap-chengdu.myqcloud.com/deepLearning_course1/%E8%AF%BE%E7%A8%8B1-%E7%AC%AC%E4%BA%8C%E5%91%A8%E4%BD%9C%E4%B8%9A1-2.png" alt></p><p>其实现代码如下所示：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">softmax</span><span class="params">(x)</span>:</span></span><br><span class="line">  x_exp = np.exp(x)</span><br><span class="line">  x_sum = np.sum(x_exp,axis=<span class="number">1</span>,keepdims = <span class="literal">True</span>)</span><br><span class="line">  <span class="keyword">return</span> x_exp/x_sum</span><br></pre></td></tr></table></figure><h1 id="2-关于两个损失函数的实现"><a href="#2-关于两个损失函数的实现" class="headerlink" title="2. 关于两个损失函数的实现"></a>2. 关于两个损失函数的实现</h1><p>在深度学习中，损失函数常用来评估模型性能，如果损失函数的值越大，代表着模型的预测值与真实值之间的差值越大，深度学习中常用梯度下降算法来优化算法以最小化损失函数。</p><h2 id="L1损失函数"><a href="#L1损失函数" class="headerlink" title="L1损失函数"></a>L1损失函数</h2><p>L1损失函数的定义如下所示：<br>$$\begin{align<em>} &amp; L_1(\hat{y}, y) = \sum_{i=0}^m|y^{(i)} - \hat{y}^{(i)}|\end{align</em>}$$</p><p>其代码实现如下所示：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">L1</span><span class="params">(y,yhat)</span>:</span></span><br><span class="line">  loss = np.sum(np.abs(y-yhat))</span><br><span class="line">  <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure><h2 id="L2损失函数"><a href="#L2损失函数" class="headerlink" title="L2损失函数"></a>L2损失函数</h2><p>L2损失函数的定义如下所示：<br>$$\begin{align<em>} &amp; L_2(\hat{y},y) = \sum_{i=0}^m(y^{(i)} -\hat{y}^{(i)})^2 \end{align</em>}$$<br><strong>注意：</strong>对于一个向量$[X] = [x_1,x_2,···x_n]$，<code>np.dot()</code>的计算过</p><p>程为：<code>np.dot(x,x) =</code>$\sum_{i=0}^{n}x_i^{2}$，所以L2损失函数的一种有效计算方式是利用<code>dot()</code>函数，具体实现代码如下所示：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">L2</span><span class="params">(y,yhat)</span>:</span></span><br><span class="line">  loss = np.sum(np.dot((y-yhat),(y-yhat).T))</span><br><span class="line">  <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;


&lt;blockquote&gt;
&lt;p&gt;deeplearning.ai官网地址：&lt;a href=&quot;https://www.deeplearning.ai/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://www.de
      
    
    </summary>
    
    
      <category term="深度学习" scheme="https://elmadavies.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="深度学习" scheme="https://elmadavies.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="神经网络" scheme="https://elmadavies.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>神经网络基础</title>
    <link href="https://elmadavies.github.io/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80/"/>
    <id>https://elmadavies.github.io/深度学习/神经网络基础/</id>
    <published>2019-10-08T10:53:22.812Z</published>
    <updated>2019-10-08T12:35:46.788Z</updated>
    
    <content type="html"><![CDATA[<p> 假设有以下一张图片，要判断其输出是否为猫，若是，则可以用$y = 1$表示，否则用$y = 0$表示。</p><p><img src="https://elamdavies-1300381401.cos.ap-chengdu.myqcloud.com/deepLearning_course1/1-1.png" alt></p><a id="more"></a><h2 id="二元分类"><a href="#二元分类" class="headerlink" title="二元分类"></a>二元分类</h2><p>计算机保存一张图片通常使用3个独立矩阵，分别对应红、绿、蓝三个颜色通道。如果输入像素是<code>64×64</code>,则会有3个<code>64×64</code>的矩阵，即输入是一个<code>3×64×64</code>的高维度数据，而输出是$y=0 \ or\  1$。</p><p>与<a href="https://www.jianshu.com/p/c18c1e08f239" target="_blank" rel="noopener">机器学习之单变量线性回归</a>一样，可以用$(x,y)$表示一个训练样本，其中$x \in R^n$,而$y = {0,1}$，通常用${(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)})……(x^{(m)},y^{(m)}) }$表示整个训练集，其中$m$表示训练样本的大小，以上，由$m$个训练样本可以组成输入矩阵$X$,且$X \in R^{n×m}$，$Y$是一个输出矩阵，且$Y \in R^{1×m}$。</p><h2 id="逻辑回归"><a href="#逻辑回归" class="headerlink" title="逻辑回归"></a>逻辑回归</h2><p>对于以上分类问题，给定输入$x$，我们想知道$y$的输出概率，$\hat{y}$的输出概率代表着该分类问题的结果，$\hat{y}$的值表示是猫的概率可以简单表示为$\hat{y} = p{y=1|x}$，且$0 \le \hat{y} \le 1$。</p><p>对于线性回归，有$y = w^Tx+b$，而对于逻辑回归，采用激活函数，即：$ y = \delta(w^Tx+b) $表示，令$z = w^Tx+b$，则：</p><p>$\delta(z) = \frac{1}{1+e^{-z}}$，其函数图像如下所示：</p><p><img src="https://elamdavies-1300381401.cos.ap-chengdu.myqcloud.com/deepLearning_course1/1-2.png" alt></p><p>当$\delta(z) &gt; 0.5$，即也就是$w^Tx+b &gt; 0 $时，认为其输出$\hat{y} = 1$<br>而$\delta(z) &lt; 0.5$，即也就是$w^Tx+b &lt; 0 $时,其输出$\hat{y} = 0$.</p><h2 id="逻辑回归损失函数"><a href="#逻辑回归损失函数" class="headerlink" title="逻辑回归损失函数"></a>逻辑回归损失函数</h2><p>对于逻辑回归问题，给定训练样本：<br>${(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)})……(x^{(m)},y^{(m)}) }$，我们希望得到$\hat{y} \approx y$。逻辑回归的损失函数$L(\hat{y},y)$可以由以下公式定义：</p><p>$L(\hat{y},y) = -ylog(\hat{y}) -(1-y)log(1-\hat{y})$</p><p>对于以上损失函数，有：<br>若$y = 1$，则$L(y,\hat{y}) = -ylog(\hat{y})$，而想让损失函数$L(y,\hat{y})$尽可能小，意味着$\hat{y}$要尽可能大，又因为$0 \le\hat{y} \le 1$，所以$\hat{y} = 1$是，损失函数最小。</p><p>若$y = 0$,则$L(y,\hat{y}) = -log(1-\hat{y})$,损失函数要取得最小值，意味着$\hat{y}$需取得最大值，则需满足$\hat{y} = 0$。</p><p>以上损失函数只适用于单个样本，对于$m$个样本的损失函数可以有如下定义：</p><p>$J(w,b) = \frac{1}{m}\sum_{i=1}^{m}L(y,\hat{y}) = \frac{1}{m}\sum_{i=1}^{m}-y^{(i)}log(\hat{y}^{(i)})-(1-y^{(i)})log(1-\hat{y}^{(i)})$</p><h2 id="梯度下降法"><a href="#梯度下降法" class="headerlink" title="梯度下降法"></a>梯度下降法</h2><p>对于以上损失函数，需要找到损失函数$J(w,b)$的最小值，最常用的算法就是梯度下降算法，即对于一个凸函数，总能通过梯度下降算法找到它的全局最优解，对于此损失函数的梯度下降算法，在<a href="https://www.jianshu.com/p/b603be6c9f0d" target="_blank" rel="noopener">机器学习之逻辑回归</a>的算法介绍中已经做了较为详细的推导，在此不再过多叙述，梯度下降算法的简单实现步骤如下所示：<br>$repeat \ \ {$<br>$w: = w- \alpha \frac{\partial J(w,b)}{\partial w}$</p><p>$}$<br>重复以上过程，直到损失函数收敛，以求得参数$w$的值，其中，$\alpha$代表学习率。</p><h2 id="计算图"><a href="#计算图" class="headerlink" title="计算图"></a>计算图</h2><h3 id="计算图介绍："><a href="#计算图介绍：" class="headerlink" title="计算图介绍："></a>计算图介绍：</h3><p>假设有一函数表达式为：$J(a,b,c) = 3(a+bc)$，其计算过程可以简单分为三个步骤，如下所示：</p><ul><li>$u = bc$</li><li>$v = a +u $</li><li>$J =3*v $<br>对于以上三个步骤，用计算图可以有如下表示</li></ul><p><img src="https://elamdavies-1300381401.cos.ap-chengdu.myqcloud.com/deepLearning_course1/1-3.png" alt></p><h3 id="计算图的导数："><a href="#计算图的导数：" class="headerlink" title="计算图的导数："></a>计算图的导数：</h3><p>如上图所示，利用计算图从左向右的流程，一步步可以算出$J$的值，那么，依靠从右向左的反向传播就可以算出$J$对每个变量的导数，如下图所示：</p><p><img src="https://elamdavies-1300381401.cos.ap-chengdu.myqcloud.com/deepLearning_course1/1-4.png" alt></p><p>其反向传播过程如图中红色箭头所示，根据导数定义以及链式计算法则，有如下计算：</p><p>$\frac{\partial J}{\partial v} = 3$</p><p>$\frac{\partial J}{\partial u} =\frac{\partial J}{\partial v} \frac{\partial v}{\partial u} = 3×1 = 3$</p><p>$\frac{\partial J}{\partial a} =\frac{\partial J}{\partial v} \frac{\partial v}{\partial a}  = 3×1 =3$</p><p>$\frac{\partial J}{\partial b} =\frac{\partial J}{\partial v} \frac{\partial v}{\partial u}  \frac{\partial u}{\partial b} = 3×1×5 =15$</p><p>$\frac{\partial J}{\partial c} =\frac{\partial J}{\partial v} \frac{\partial v}{\partial u}  \frac{\partial u}{\partial c} = 3×1×4 =12$</p><h2 id="逻辑回归中的梯度下降算法"><a href="#逻辑回归中的梯度下降算法" class="headerlink" title="逻辑回归中的梯度下降算法"></a>逻辑回归中的梯度下降算法</h2><h3 id="单个样本的逻辑回归梯度下降算法"><a href="#单个样本的逻辑回归梯度下降算法" class="headerlink" title="单个样本的逻辑回归梯度下降算法"></a>单个样本的逻辑回归梯度下降算法</h3><p>关于逻辑回归的损失函数，有如下公式：<br>$z= w^Tx+b$<br>$\hat{y} = a = \delta(z)$<br>$L(a,y) =  -ylog(a)-(1-y)log(1-a)$<br>假设有两个输入特征$x_1,x_2$和两个参数$w_1,w_2$,则用计算图（流程图）表示其计算过程如下所示：</p><p><img src="https://elamdavies-1300381401.cos.ap-chengdu.myqcloud.com/deepLearning_course1/1-5.png" alt></p><p>依照其计算图中的反向传播过程和链式法则，其导数计算如下所示：</p><p>$\frac{\partial L(a,y)}{\partial a} = -\frac{y}{a}+\frac{1-y}{1-a}$</p><p>$\frac{\partial L(a,y)}{\partial z} = \frac{\partial L}{\partial a} \frac{\partial a}{\partial z} = a-y$</p><p>$\frac{\partial L(a,y)}{\partial w_1} =\frac{\partial L}{\partial a} \frac{\partial a}{\partial z} \frac{\partial z}{\partial w_1} =x_1dz = x_1*(a-y)$</p><p>$\frac{\partial L(a,y)}{\partial w_2} =\frac{\partial L}{\partial a} \frac{\partial a}{\partial z} \frac{\partial z}{\partial w_2} = x_2dz = x_2*(a-y)$</p><p>$\frac{\partial L(a,y)}{\partial b} =\frac{\partial L}{\partial a} \frac{\partial a}{\partial z} \frac{\partial z}{\partial b} = dz = (a-y)$<br>···</p><p>最后，参数$w_1,w_2,b$的更新规律为：<br>$w_1 := w_1  - \alpha dw_1$<br>$w_2 := w_2  - \alpha dw_2$<br>$b := b  - \alpha db$<br>其中，$\alpha$表示学习率。</p><h3 id="m-个样本的逻辑回归"><a href="#m-个样本的逻辑回归" class="headerlink" title="$m$个样本的逻辑回归"></a>$m$个样本的逻辑回归</h3><p>$m$个样本的损失函数，如下所示：</p><p>$J(w,b) = \frac{1}{m}\sum_{i=1}^{m}L(a^{(i)},y^{(i)}) $</p><p>$a^{(i)} = \hat{y}^{(i)} = \delta(z^{(i)}) = \delta(w^Tx^{(i)} +b)$</p><p>其梯度计算公式，可以有如下表示：<br>$\frac{\partial J(w,b)}{\partial w} = \frac{1}<br>{m}\sum_{i=1}^{m}\frac{\partial }{\partial w}L(a^{(i)},y^{(i)}) $</p><p>在实际计算过程中，需要计算每一个样本的关于$w$的梯度，最后求和取平均，在一个具体算法实现中，其为代码可以如下所示：</p><p>假设有2个特征向量，$m$个样本，则有：<br>初始化：$J = 0, dw_1 = 0,dw_2 = 0$<br>$for  i =1  to \ \ m:$</p><p>$ \ \ \ \ \ \ z^{(i)} = w^Tx^{(i)} +b;$</p><p>$ \ \ \ \ \ \ a^{(i)} =\delta(z^{(i)});$</p><p>$ \ \ \ \ \ \ J+ =  -y^{(i)}log(a^{(i)})-(1-y^{(i)})log(1-a^{(i)});$</p><p>$ \ \ \ \ \ \ dz^{(i)} =a^{(i)} - y^{(i)};$</p><p>$ \ \ \ \ \ \ dw_1 +=x_1*dz^{(i)};$</p><p>$ \ \ \ \ \ \ dw_2 +=x_2*dz^{(i)};$</p><p>$ \ \ \ \ \ \ db \ +=dz^{(i)};$</p><p>$J/=m,dw_1/=m,dw_2/=m,db/=m;$</p><p>以上，是应用一次梯度下降的过程，应用多次梯度下降算法之后，其参数的更新如下所示：<br>$w_1 := w_1  - \alpha dw_1$<br>$w_2 := w_2  - \alpha dw_2$<br>$b := b  - \alpha db$</p><blockquote><p>注意：以上，算法实现过程中，有两个特征和参数，分别是$x_1,x_2$和$w_1,w_2$,当有$n$个特征和参数时，可以利用循环完成。</p></blockquote><h2 id="向量化"><a href="#向量化" class="headerlink" title="向量化"></a>向量化</h2><h3 id="向量化的简单示例："><a href="#向量化的简单示例：" class="headerlink" title="向量化的简单示例："></a>向量化的简单示例：</h3><p>如以上算法表示，通过<code>for</code>循环来遍历$m$个样本和$n$个特征，当在整个算法运行过程中，需要考虑运行时间的问题，当样本数量和特征足够大时，采用传统的<code>for</code>循环不是一个明智的选择，为了减少算法运行时间，特地引入了向量化的实现。<br>将一以下代码作为示例：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line">a = np.random.rand(<span class="number">1000000</span>)</span><br><span class="line">b = np.random.rand(<span class="number">1000000</span>)</span><br><span class="line">ts = time.time()</span><br><span class="line">c = np.dot(a,b)</span><br><span class="line">te = time.time()</span><br><span class="line">print(c)</span><br><span class="line">print(<span class="string">"向量化的代码实现花费时间："</span>+str((te-ts)*<span class="number">1000</span>)+<span class="string">" ms"</span>)</span><br><span class="line"></span><br><span class="line">c = <span class="number">0</span></span><br><span class="line">ts = time.time()</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1000000</span>):</span><br><span class="line">    c += a[i]*b[i]</span><br><span class="line">te = time.time()</span><br><span class="line">print(c)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"for循环代码实现花费时间："</span>+str((te-ts)*<span class="number">1000</span>)+<span class="string">" ms"</span>)</span><br></pre></td></tr></table></figure><p><img src="https://elamdavies-1300381401.cos.ap-chengdu.myqcloud.com/deepLearning_course1/1-6.png" alt></p><p>如上所示，同样实现两个数组（向量）相乘的过程，对于百万级别的数据，<code>for</code>循环的实现方式所花费的时间差不多是向量化的400倍左右，向量化的实现可以简单的理解为是一个并行的过程，而<code>for</code>循环可以简单理解为串行的过程，所以通过向量化的实现，大大节省了运行程序所耗费的时间。在算法实现过程中，应该尽量避免使用<code>for</code>循环。</p><h3 id="用向量化实现逻辑回归："><a href="#用向量化实现逻辑回归：" class="headerlink" title="用向量化实现逻辑回归："></a>用向量化实现逻辑回归：</h3><p>对于逻辑回归的算法，需要考虑输入向量$X$和权重参数$W$，其中，$X \in R^{n×m}$，$W \in R^{n×1}$，而根据矩阵乘法运算法则和逻辑回归的实现原理，有：<br>$[z_1,z_2,……z_m] = [W^TX]+[b_1,b_2,……b_m]$</p><p>在<code>python</code>中用<code>numpy</code>库，可以简单的用以下一行代码实现（一般认为$b$是一个R^{1×1}的偏置常量）：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">z = np.dot(W.T,x)+b</span><br></pre></td></tr></table></figure><p>根据之前的学习，对于逻辑回归利用反向传播算法计算导数，有:</p><blockquote><p>$ \ \ \ \ \ \ dz^{(i)} =a^{(i)} - y^{(i)};$<br>$ \ \ \ \ \ \ dw_1 +=x_1<em>dz^{(i)};$<br>$ \ \ \ \ \ \ dw_2 +=x_2</em>dz^{(i)};$<br>$\ \ \ \ \ \ \ …$<br>$ \ \ \ \ \ \ dw_n +=x_n*dz^{(i)};$<br>$ \ \ \ \ \ \ db \ +=dz^{(i)};$<br>$J/=m,dw_1/=m,dw_2/=m,db/=m;$</p></blockquote><p>对于以上，公式，有如下定义：<br>$dZ = [dz^{(1)},dz^{(2)}……dz^{(m)}]$<br>$A = [a^{(1)},a^{(2)},……a^{(m)}]$<br>$Y = [y^{(1)},y^{(2)}……y^{(m)}]$<br>$dZ = [A - Y]$<br>对于以上过程，摈弃传统的<code>for</code>循环实现，采用向量化的实现方式可以简单表示为：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">dw = np.dot(X,dZ^T)</span><br><span class="line">db = <span class="number">1</span>/m*np.sum(dZ)</span><br></pre></td></tr></table></figure><p>综合以上所有向量化的实现，可以得到利用<code>python</code>实现的一个高度向量化的逻辑回归梯度下降算法（a代表学习率）：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Z = np.dot(W^T,X)+b</span><br><span class="line">A = np.exp(Z)</span><br><span class="line">dZ = A-Y</span><br><span class="line">dw = np.dot(X,dZ^T)</span><br><span class="line">db = <span class="number">1</span>/m*np.sum(dZ)</span><br><span class="line">w = w-a*dw</span><br><span class="line">b = b-a*db</span><br></pre></td></tr></table></figure><p>以上，只是实现一次梯度下降的伪代码，在实际算法运行过程中，我们仍然需要利用循环实现多次梯度下降。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt; 假设有以下一张图片，要判断其输出是否为猫，若是，则可以用$y = 1$表示，否则用$y = 0$表示。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://elamdavies-1300381401.cos.ap-chengdu.myqcloud.com/deepLearning_course1/1-1.png&quot; alt&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="深度学习" scheme="https://elmadavies.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="深度学习" scheme="https://elmadavies.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="神经网络" scheme="https://elmadavies.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>新的开始</title>
    <link href="https://elmadavies.github.io/%E9%9A%8F%E7%AC%94/%E6%96%B0%E7%9A%84%E5%BC%80%E5%A7%8B/"/>
    <id>https://elmadavies.github.io/随笔/新的开始/</id>
    <published>2019-10-07T12:59:24.661Z</published>
    <updated>2019-10-07T14:25:12.147Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://elamdavies-1300381401.cos.ap-chengdu.myqcloud.com/blog_background/bg1.jpg" alt></p><a id="more"></a><h4 id="一些说明"><a href="#一些说明" class="headerlink" title="一些说明"></a>一些说明</h4><p>花了一个国庆的时间，终于将这个博客搭建的差不多了，尽管还有很多不足，但是目前基本够用，也许在使用过程中还会出现很多bug，只能边用边解决了。<br><br><br>搭建一个自己的博客是我一直想要做的事，但是之前由于被各种繁杂的事务耽搁，导致一直没有时间动手去做，趁着这个国庆假期终于做好了。整个博客是用<code>hexo+github</code>搭建完成的，倒也没什么难度，就是需要一些耐心，当然，如果有一些前端方面的知识，搭建起来会更加顺利，可定制化也会更高。<br><br><br>以后，会慢慢的将自己的简书上的文章迁移过来，以后也会在简书平台和这个博客网站共同记录自己的学习笔记，除此之外，博客平台除了技术也会记录一些自己的生活经历等。<br><br><br>在搭建博客的过程中，看到可好多大佬的博客非常漂亮，也受到了好多大佬的所写文章的指点，在此表示非常感谢。为此，附上几个教学链接，有需要搭建此类博客的同学，也可以了解一下。</p><h4 id="博客搭建与优化教程"><a href="#博客搭建与优化教程" class="headerlink" title="博客搭建与优化教程"></a>博客搭建与优化教程</h4><ul><li><p><a href="http://yearito.cn/categories/%E6%8A%80%E6%9C%AF/%E5%8D%9A%E5%AE%A2/" target="_blank" rel="noopener">搭建教程1</a></p></li><li><p><a href="https://bestzuo.cn/categories/%E5%8D%9A%E5%AE%A2/" target="_blank" rel="noopener">搭建教程2</a></p></li><li><p><a href="https://bestzuo.cn/categories/%E5%8D%9A%E5%AE%A2/" target="_blank" rel="noopener">搭建教程3</a></p></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;https://elamdavies-1300381401.cos.ap-chengdu.myqcloud.com/blog_background/bg1.jpg&quot; alt&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="随笔" scheme="https://elmadavies.github.io/categories/%E9%9A%8F%E7%AC%94/"/>
    
    
      <category term="Start" scheme="https://elmadavies.github.io/tags/Start/"/>
    
  </entry>
  
</feed>
