<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>此间不留白</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://elmadavies.github.io/"/>
  <updated>2019-10-08T11:22:05.087Z</updated>
  <id>https://elmadavies.github.io/</id>
  
  <author>
    <name>ElmaDavies</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>神经网络基础</title>
    <link href="https://elmadavies.github.io/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80/"/>
    <id>https://elmadavies.github.io/深度学习/神经网络基础/</id>
    <published>2019-10-08T10:53:22.812Z</published>
    <updated>2019-10-08T11:22:05.087Z</updated>
    
    <content type="html"><![CDATA[<p> 假设有以下一张图片，要判断其输出是否为猫，若是，则可以用$y = 1$表示，否则用$y = 0$表示。</p><p><img src="https://elamdavies-1300381401.cos.ap-chengdu.myqcloud.com/deepLearning_course1/1-1.png" alt></p><a id="more"></a><h2 id="二元分类"><a href="#二元分类" class="headerlink" title="二元分类"></a>二元分类</h2><p>计算机保存一张图片通常使用3个独立矩阵，分别对应红、绿、蓝三个颜色通道。如果输入像素是<code>64×64</code>,则会有3个<code>64×64</code>的矩阵，即输入是一个<code>3×64×64</code>的高维度数据，而输出是$y=0 \ or\  1$。</p><p>与<a href="https://www.jianshu.com/p/c18c1e08f239" target="_blank" rel="noopener">机器学习之单变量线性回归</a>一样，可以用$(x,y)$表示一个训练样本，其中$x \in R^n$,而$y = {0,1}$，通常用${(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)})……(x^{(m)},y^{(m)}) }$表示整个训练集，其中$m$表示训练样本的大小，以上，由$m$个训练样本可以组成输入矩阵$X$,且$X \in R^{n×m}$，$Y$是一个输出矩阵，且$Y \in R^{1×m}$。</p><h2 id="逻辑回归"><a href="#逻辑回归" class="headerlink" title="逻辑回归"></a>逻辑回归</h2><p>对于以上分类问题，给定输入$x$，我们想知道$y$的输出概率，$\hat{y}$的输出概率代表着该分类问题的结果，$\hat{y}$的值表示是猫的概率可以简单表示为$\hat{y} = p{y=1|x}$，且$0 \le \hat{y} \le 1$。</p><p>对于线性回归，有$y = w^Tx+b$，而对于逻辑回归，采用激活函数，即：$ y = \delta(w^Tx+b) $表示，令$z = w^Tx+b$，则：</p><p>$\delta(z) = \frac{1}{1+e^{-z}}$，其函数图像如下所示：</p><p><img src="https://elamdavies-1300381401.cos.ap-chengdu.myqcloud.com/deepLearning_course1/1-2.png" alt></p><p>当$\delta(z) &gt; 0.5$，即也就是$w^Tx+b &gt; 0 $时，认为其输出$\hat{y} = 1$<br>而$\delta(z) &lt; 0.5$，即也就是$w^Tx+b &lt; 0 $时,其输出$\hat{y} = 0$.</p><h2 id="逻辑回归损失函数"><a href="#逻辑回归损失函数" class="headerlink" title="逻辑回归损失函数"></a>逻辑回归损失函数</h2><p>对于逻辑回归问题，给定训练样本：<br>${(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)})……(x^{(m)},y^{(m)}) }$，我们希望得到$\hat{y} \approx y$。逻辑回归的损失函数$L(\hat{y},y)$可以由以下公式定义：</p><p>$L(\hat{y},y) = -ylog(\hat{y}) -(1-y)log(1-\hat{y})$</p><p>对于以上损失函数，有：<br>若$y = 1$，则$L(y,\hat{y}) = -ylog(\hat{y})$，而想让损失函数$L(y,\hat{y})$尽可能小，意味着$\hat{y}$要尽可能大，又因为$0 \le\hat{y} \le 1$，所以$\hat{y} = 1$是，损失函数最小。</p><p>若$y = 0$,则$L(y,\hat{y}) = -log(1-\hat{y})$,损失函数要取得最小值，意味着$\hat{y}$需取得最大值，则需满足$\hat{y} = 0$。</p><p>以上损失函数只适用于单个样本，对于$m$个样本的损失函数可以有如下定义：</p><p>$J(w,b) = \frac{1}{m}\sum_{i=1}^{m}L(y,\hat{y}) = \frac{1}{m}\sum_{i=1}^{m}-y^{(i)}log(\hat{y}^{(i)})-(1-y^{(i)})log(1-\hat{y}^{(i)})$</p><h2 id="梯度下降法"><a href="#梯度下降法" class="headerlink" title="梯度下降法"></a>梯度下降法</h2><p>对于以上损失函数，需要找到损失函数$J(w,b)$的最小值，最常用的算法就是梯度下降算法，即对于一个凸函数，总能通过梯度下降算法找到它的全局最优解，对于此损失函数的梯度下降算法，在<a href="https://www.jianshu.com/p/b603be6c9f0d" target="_blank" rel="noopener">机器学习之逻辑回归</a>的算法介绍中已经做了较为详细的推导，在此不再过多叙述，梯度下降算法的简单实现步骤如下所示：<br>$repeat \ \ {$<br>$w: = w- \alpha \frac{\partial J(w,b)}{\partial w}$</p><p>$}$<br>重复以上过程，直到损失函数收敛，以求得参数$w$的值，其中，$\alpha$代表学习率。</p><h2 id="计算图"><a href="#计算图" class="headerlink" title="计算图"></a>计算图</h2><h3 id="计算图介绍："><a href="#计算图介绍：" class="headerlink" title="计算图介绍："></a>计算图介绍：</h3><p>假设有一函数表达式为：$J(a,b,c) = 3(a+bc)$，其计算过程可以简单分为三个步骤，如下所示：</p><ul><li>$u = bc$</li><li>$v = a +u $</li><li>$J =3*v $<br>对于以上三个步骤，用计算图可以有如下表示</li></ul><p><img src="https://elamdavies-1300381401.cos.ap-chengdu.myqcloud.com/deepLearning_course1/1-3.png" alt></p><h3 id="计算图的导数："><a href="#计算图的导数：" class="headerlink" title="计算图的导数："></a>计算图的导数：</h3><p>如上图所示，利用计算图从左向右的流程，一步步可以算出$J$的值，那么，依靠从右向左的反向传播就可以算出$J$对每个变量的导数，如下图所示：</p><p><img src="https://elamdavies-1300381401.cos.ap-chengdu.myqcloud.com/deepLearning_course1/1-4.png" alt></p><p>其反向传播过程如图中红色箭头所示，根据导数定义以及链式计算法则，有如下计算：</p><p>$\frac{\partial J}{\partial v} = 3$</p><p>$\frac{\partial J}{\partial u} =\frac{\partial J}{\partial v} \frac{\partial v}{\partial u} = 3×1 = 3$</p><p>$\frac{\partial J}{\partial a} =\frac{\partial J}{\partial v} \frac{\partial v}{\partial a}  = 3×1 =3$</p><p>$\frac{\partial J}{\partial b} =\frac{\partial J}{\partial v} \frac{\partial v}{\partial u}  \frac{\partial u}{\partial b} = 3×1×5 =15$</p><p>$\frac{\partial J}{\partial c} =\frac{\partial J}{\partial v} \frac{\partial v}{\partial u}  \frac{\partial u}{\partial c} = 3×1×4 =12$</p><h2 id="逻辑回归中的梯度下降算法"><a href="#逻辑回归中的梯度下降算法" class="headerlink" title="逻辑回归中的梯度下降算法"></a>逻辑回归中的梯度下降算法</h2><h3 id="单个样本的逻辑回归梯度下降算法"><a href="#单个样本的逻辑回归梯度下降算法" class="headerlink" title="单个样本的逻辑回归梯度下降算法"></a>单个样本的逻辑回归梯度下降算法</h3><p>关于逻辑回归的损失函数，有如下公式：<br>$z= w^Tx+b$<br>$\hat{y} = a = \delta(z)$<br>$L(a,y) =  -ylog(a)-(1-y)log(1-a)$<br>假设有两个输入特征$x_1,x_2$和两个参数$w_1,w_2$,则用计算图（流程图）表示其计算过程如下所示：</p><p><img src="https://elamdavies-1300381401.cos.ap-chengdu.myqcloud.com/deepLearning_course1/1-5.png" alt></p><p>依照其计算图中的反向传播过程和链式法则，其导数计算如下所示：</p><p>$\frac{\partial L(a,y)}{\partial a} = -\frac{y}{a}+\frac{1-y}{1-a}$</p><p>$\frac{\partial L(a,y)}{\partial z} = \frac{\partial L}{\partial a} \frac{\partial a}{\partial z} = a-y$</p><p>$\frac{\partial L(a,y)}{\partial w_1} =\frac{\partial L}{\partial a} \frac{\partial a}{\partial z} \frac{\partial z}{\partial w_1} =x_1dz = x_1*(a-y)$</p><p>$\frac{\partial L(a,y)}{\partial w_2} =\frac{\partial L}{\partial a} \frac{\partial a}{\partial z} \frac{\partial z}{\partial w_2} = x_2dz = x_2*(a-y)$</p><p>$\frac{\partial L(a,y)}{\partial b} =\frac{\partial L}{\partial a} \frac{\partial a}{\partial z} \frac{\partial z}{\partial b} = dz = (a-y)$<br>···</p><p>最后，参数$w_1,w_2,b$的更新规律为：<br>$w_1 := w_1  - \alpha dw_1$<br>$w_2 := w_2  - \alpha dw_2$<br>$b := b  - \alpha db$<br>其中，$\alpha$表示学习率。</p><h3 id="m-个样本的逻辑回归"><a href="#m-个样本的逻辑回归" class="headerlink" title="$m$个样本的逻辑回归"></a>$m$个样本的逻辑回归</h3><p>$m$个样本的损失函数，如下所示：</p><p>$J(w,b) = \frac{1}{m}\sum_{i=1}^{m}L(a^{(i)},y^{(i)}) $</p><p>$a^{(i)} = \hat{y}^{(i)} = \delta(z^{(i)}) = \delta(w^Tx^{(i)} +b)$</p><p>其梯度计算公式，可以有如下表示：<br>$\frac{\partial J(w,b)}{\partial w} = \frac{1}<br>{m}\sum_{i=1}^{m}\frac{\partial }{\partial w}L(a^{(i)},y^{(i)}) $</p><p>在实际计算过程中，需要计算每一个样本的关于$w$的梯度，最后求和取平均，在一个具体算法实现中，其为代码可以如下所示：</p><p>假设有2个特征向量，$m$个样本，则有：<br>初始化：$J = 0, dw_1 = 0,dw_2 = 0$<br>$for \ \ i =1 \ \ to \ \ m:$</p><p>$ \ \ \ \ \ \ z^{(i)} = w^Tx^{(i)} +b;$</p><p>$ \ \ \ \ \ \ a^{(i)} =\delta(z^{(i)});$</p><p>$ \ \ \ \ \ \ J+ =  -y^{(i)}log(a^{(i)})-(1-y^{(i)})log(1-a^{(i)});$</p><p>$ \ \ \ \ \ \ dz^{(i)} =a^{(i)} - y^{(i)};$</p><p>$ \ \ \ \ \ \ dw_1 +=x_1*dz^{(i)};$</p><p>$ \ \ \ \ \ \ dw_2 +=x_2*dz^{(i)};$</p><p>$ \ \ \ \ \ \ db \ +=dz^{(i)};$</p><p>$J/=m,dw_1/=m,dw_2/=m,db/=m;$</p><p>以上，是应用一次梯度下降的过程，应用多次梯度下降算法之后，其参数的更新如下所示：<br>$w_1 := w_1  - \alpha dw_1$<br>$w_2 := w_2  - \alpha dw_2$<br>$b := b  - \alpha db$</p><blockquote><p>注意：以上，算法实现过程中，有两个特征和参数，分别是$x_1,x_2$和$w_1,w_2$,当有$n$个特征和参数时，可以利用循环完成。</p></blockquote><h2 id="向量化"><a href="#向量化" class="headerlink" title="向量化"></a>向量化</h2><h3 id="向量化的简单示例："><a href="#向量化的简单示例：" class="headerlink" title="向量化的简单示例："></a>向量化的简单示例：</h3><p>如以上算法表示，通过<code>for</code>循环来遍历$m$个样本和$n$个特征，当在整个算法运行过程中，需要考虑运行时间的问题，当样本数量和特征足够大时，采用传统的<code>for</code>循环不是一个明智的选择，为了减少算法运行时间，特地引入了向量化的实现。<br>将一以下代码作为示例：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line">a = np.random.rand(<span class="number">1000000</span>)</span><br><span class="line">b = np.random.rand(<span class="number">1000000</span>)</span><br><span class="line">ts = time.time()</span><br><span class="line">c = np.dot(a,b)</span><br><span class="line">te = time.time()</span><br><span class="line">print(c)</span><br><span class="line">print(<span class="string">"向量化的代码实现花费时间："</span>+str((te-ts)*<span class="number">1000</span>)+<span class="string">" ms"</span>)</span><br><span class="line"></span><br><span class="line">c = <span class="number">0</span></span><br><span class="line">ts = time.time()</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1000000</span>):</span><br><span class="line">    c += a[i]*b[i]</span><br><span class="line">te = time.time()</span><br><span class="line">print(c)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"for循环代码实现花费时间："</span>+str((te-ts)*<span class="number">1000</span>)+<span class="string">" ms"</span>)</span><br></pre></td></tr></table></figure><p><img src="https://elamdavies-1300381401.cos.ap-chengdu.myqcloud.com/deepLearning_course1/1-6.png" alt></p><p>如上所示，同样实现两个数组（向量）相乘的过程，对于百万级别的数据，<code>for</code>循环的实现方式所花费的时间差不多是向量化的400倍左右，向量化的实现可以简单的理解为是一个并行的过程，而<code>for</code>循环可以简单理解为串行的过程，所以通过向量化的实现，大大节省了运行程序所耗费的时间。在算法实现过程中，应该尽量避免使用<code>for</code>循环。</p><h3 id="用向量化实现逻辑回归："><a href="#用向量化实现逻辑回归：" class="headerlink" title="用向量化实现逻辑回归："></a>用向量化实现逻辑回归：</h3><p>对于逻辑回归的算法，需要考虑输入向量$X$和权重参数$W$，其中，$X \in R^{n×m}$，$W \in R^{n×1}$，而根据矩阵乘法运算法则和逻辑回归的实现原理，有：<br>$[z_1,z_2,……z_m] = [W^TX]+[b_1,b_2,……b_m]$</p><p>在<code>python</code>中用<code>numpy</code>库，可以简单的用以下一行代码实现（一般认为$b$是一个R^{1×1}的偏置常量）：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">z = np.dot(W.T,x)+b</span><br></pre></td></tr></table></figure><p>根据之前的学习，对于逻辑回归利用反向传播算法计算导数，有:</p><blockquote><p>$ \ \ \ \ \ \ dz^{(i)} =a^{(i)} - y^{(i)};$<br>$ \ \ \ \ \ \ dw_1 +=x_1<em>dz^{(i)};$<br>$ \ \ \ \ \ \ dw_2 +=x_2</em>dz^{(i)};$<br>$\ \ \ \ \ \ \ …$<br>$ \ \ \ \ \ \ dw_n +=x_n*dz^{(i)};$<br>$ \ \ \ \ \ \ db \ +=dz^{(i)};$<br>$J/=m,dw_1/=m,dw_2/=m,db/=m;$</p></blockquote><p>对于以上，公式，有如下定义：<br>$dZ = [dz^{(1)},dz^{(2)}……dz^{(m)}]$<br>$A = [a^{(1)},a^{(2)},……a^{(m)}]$<br>$Y = [y^{(1)},y^{(2)}……y^{(m)}]$<br>$dZ = [A - Y]$<br>对于以上过程，摈弃传统的<code>for</code>循环实现，采用向量化的实现方式可以简单表示为：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">dw = np.dot(X,dZ^T)</span><br><span class="line">db = <span class="number">1</span>/m*np.sum(dZ)</span><br></pre></td></tr></table></figure><p>综合以上所有向量化的实现，可以得到利用<code>python</code>实现的一个高度向量化的逻辑回归梯度下降算法（a代表学习率）：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Z = np.dot(W^T,X)+b</span><br><span class="line">A = np.exp(Z)</span><br><span class="line">dZ = A-Y</span><br><span class="line">dw = np.dot(X,dZ^T)</span><br><span class="line">db = <span class="number">1</span>/m*np.sum(dZ)</span><br><span class="line">w = w-a*dw</span><br><span class="line">b = b-a*db</span><br></pre></td></tr></table></figure><p>以上，只是实现一次梯度下降的伪代码，在实际算法运行过程中，我们仍然需要利用循环实现多次梯度下降。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt; 假设有以下一张图片，要判断其输出是否为猫，若是，则可以用$y = 1$表示，否则用$y = 0$表示。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://elamdavies-1300381401.cos.ap-chengdu.myqcloud.com/deepLearning_course1/1-1.png&quot; alt&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="深度学习" scheme="https://elmadavies.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="深度学习" scheme="https://elmadavies.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="神经网络" scheme="https://elmadavies.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>新的开始</title>
    <link href="https://elmadavies.github.io/%E9%9A%8F%E7%AC%94/%E6%96%B0%E7%9A%84%E5%BC%80%E5%A7%8B/"/>
    <id>https://elmadavies.github.io/随笔/新的开始/</id>
    <published>2019-10-07T12:59:24.661Z</published>
    <updated>2019-10-07T14:25:12.147Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://elamdavies-1300381401.cos.ap-chengdu.myqcloud.com/blog_background/bg1.jpg" alt></p><a id="more"></a><h4 id="一些说明"><a href="#一些说明" class="headerlink" title="一些说明"></a>一些说明</h4><p>花了一个国庆的时间，终于将这个博客搭建的差不多了，尽管还有很多不足，但是目前基本够用，也许在使用过程中还会出现很多bug，只能边用边解决了。<br><br><br>搭建一个自己的博客是我一直想要做的事，但是之前由于被各种繁杂的事务耽搁，导致一直没有时间动手去做，趁着这个国庆假期终于做好了。整个博客是用<code>hexo+github</code>搭建完成的，倒也没什么难度，就是需要一些耐心，当然，如果有一些前端方面的知识，搭建起来会更加顺利，可定制化也会更高。<br><br><br>以后，会慢慢的将自己的简书上的文章迁移过来，以后也会在简书平台和这个博客网站共同记录自己的学习笔记，除此之外，博客平台除了技术也会记录一些自己的生活经历等。<br><br><br>在搭建博客的过程中，看到可好多大佬的博客非常漂亮，也受到了好多大佬的所写文章的指点，在此表示非常感谢。为此，附上几个教学链接，有需要搭建此类博客的同学，也可以了解一下。</p><h4 id="博客搭建与优化教程"><a href="#博客搭建与优化教程" class="headerlink" title="博客搭建与优化教程"></a>博客搭建与优化教程</h4><ul><li><p><a href="http://yearito.cn/categories/%E6%8A%80%E6%9C%AF/%E5%8D%9A%E5%AE%A2/" target="_blank" rel="noopener">搭建教程1</a></p></li><li><p><a href="https://bestzuo.cn/categories/%E5%8D%9A%E5%AE%A2/" target="_blank" rel="noopener">搭建教程2</a></p></li><li><p><a href="https://bestzuo.cn/categories/%E5%8D%9A%E5%AE%A2/" target="_blank" rel="noopener">搭建教程3</a></p></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;https://elamdavies-1300381401.cos.ap-chengdu.myqcloud.com/blog_background/bg1.jpg&quot; alt&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="随笔" scheme="https://elmadavies.github.io/categories/%E9%9A%8F%E7%AC%94/"/>
    
    
      <category term="Start" scheme="https://elmadavies.github.io/tags/Start/"/>
    
  </entry>
  
</feed>
